{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from mediapipe import solutions\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Video indexes for the correct and incorrect ones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_index_corr = [1, 2, 3, 4, 5, 6, 7, 47, 78, 79, 80, 83, 85, 100, 101, 102, 113, 114, 115, 116, 127, 129, 131, 132, 133\n",
    "                , 134, 135, 136, 137, 138, 140, 141, 142, 144, 146, 147, 148, 162, 163, 164, 165, 173, 174, 175, 177, 178, 186, 187, 188, 197]\n",
    "\n",
    "video_index_incorr = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 36, 37, 42, 44,\n",
    "                46, 55,56,57,58,81,104,105, 107, 108, 110, 149, 150, 151, 152, 153, 155, 156, 166, 167, 168, 171, 198]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dataset with landamarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image processing completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "\n",
    "# Define the pose landmarking model\n",
    "model_path = 'pose_landmarker_heavy.task'\n",
    "\n",
    "## These paths are not correct right now\n",
    "# TODO - Automatize this\n",
    "\n",
    "# Path to the folder containing the images\n",
    "image_folder = 'test_dataset/correct_seq/'#'images/wrong_seq/'\n",
    "\n",
    "# Define the output folder to save the processed images\n",
    "output_folder = 'test_dataset_with_landmarks/correct_seq'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "BaseOptions = mp.tasks.BaseOptions\n",
    "PoseLandmarker = mp.tasks.vision.PoseLandmarker\n",
    "PoseLandmarkerOptions = mp.tasks.vision.PoseLandmarkerOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "options = PoseLandmarkerOptions(\n",
    "    base_options=BaseOptions(model_asset_path=model_path),\n",
    "    running_mode=VisionRunningMode.IMAGE)\n",
    "\n",
    "with PoseLandmarker.create_from_options(options) as landmarker:\n",
    "    # Iterate over the images in the folder\n",
    "    for filename in os.listdir(image_folder):\n",
    "        # Read the image using OpenCV\n",
    "        image_path = os.path.join(image_folder, filename)\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        # Perform pose landmarking on the image\n",
    "        image = mp.Image.create_from_file(image_path)\n",
    "        pose_landmarker_result = landmarker.detect(image)\n",
    "\n",
    "        pose_landmarks_list = pose_landmarker_result.pose_landmarks\n",
    "\n",
    "        # Check if pose landmarks are detected\n",
    "        if pose_landmarks_list is not None:\n",
    "            annotated_image = np.copy(image.numpy_view())\n",
    "            # Loop through the detected poses to visualize.\n",
    "            for idx in range(len(pose_landmarks_list)):\n",
    "                pose_landmarks = pose_landmarks_list[idx]\n",
    "\n",
    "                # Draw the pose landmarks.\n",
    "                pose_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "                pose_landmarks_proto.landmark.extend([\n",
    "                    landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in pose_landmarks\n",
    "                ])\n",
    "\n",
    "                solutions.drawing_utils.draw_landmarks(\n",
    "                    annotated_image,\n",
    "                    pose_landmarks_proto,\n",
    "                    solutions.pose.POSE_CONNECTIONS,\n",
    "                    solutions.drawing_styles.get_default_pose_landmarks_style())\n",
    "\n",
    "            # Save the processed image to the output folder\n",
    "            output_path = os.path.join(output_folder, filename)\n",
    "            plt.imsave(output_path, annotated_image)\n",
    "        else:\n",
    "            print(f'No pose landmarks detected in {filename}.')\n",
    "\n",
    "print('Image processing completed.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean and Std deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images_with_landmarks/train/up\n",
      "Mean: 0.49097234\n",
      "Standard Deviation: 0.29159072\n",
      "images_with_landmarks/train/down\n",
      "Mean: 0.49058145\n",
      "Standard Deviation: 0.3018868\n"
     ]
    }
   ],
   "source": [
    "def get_mean_std(train_dir):\n",
    "    # Create the training dataset\n",
    "    train_data = datasets.ImageFolder(train_dir, transform=transforms.ToTensor())\n",
    "\n",
    "    # Iterate over the dataset and collect pixel values\n",
    "    pixel_values = []\n",
    "    for image, _ in train_data:\n",
    "        image = np.array(image)  # Convert the image to a NumPy array\n",
    "        pixel_values.append(image.flatten())\n",
    "\n",
    "    # Calculate mean and standard deviation\n",
    "    pixel_values = np.concatenate(pixel_values, axis=0)\n",
    "    mean = np.mean(pixel_values, axis=0)\n",
    "    std = np.std(pixel_values, axis=0)\n",
    "\n",
    "    print(train_dir)\n",
    "    print(\"Mean:\", mean)\n",
    "    print(\"Standard Deviation:\", std)\n",
    "\n",
    "train_dir_up = \"images_with_landmarks/train/up\"  # Replace with the path to your training data\n",
    "train_dir_down = \"images_with_landmarks/train/down\"\n",
    "\n",
    "get_mean_std(train_dir_up)\n",
    "get_mean_std(train_dir_down)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data augmentation and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms_up = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.49097234, 0.49097234, 0.49097234], std=[0.29159072, 0.29159072, 0.29159072])\n",
    "])\n",
    "\n",
    "train_transforms_down = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.49058145, 0.49058145, 0.49058145], std=[0.3018868, 0.3018868, 0.3018868])\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save or load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the state of the model\n",
    "def save_model(model, name):\n",
    "    torch.save(model.state_dict(), name)\n",
    "\n",
    "#model.load_state_dict(torch.load('models/resnet18_aug.pth'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Model - Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/15], Loss: 0.4935\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained model (e.g., ResNet)\n",
    "model_up = models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze all the layers in the model\n",
    "for param in model_up.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the final layer of the model to match the number of classes in the new dataset\n",
    "num_ftrs = model_up.fc.in_features\n",
    "model_up.fc = torch.nn.Linear(num_ftrs, 2)  # num_classes should be the number of classes in your new dataset\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_up.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "train_dir = \"images_with_landmarks/train/up\"  # replace train_dir with the path to your data\n",
    "\n",
    "train_data = datasets.ImageFolder(train_dir, transform=train_transforms_up)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)  # reduce batch_size\n",
    "\n",
    "\n",
    "# Training the model\n",
    "num_epochs = 15\n",
    "train_loss_history = []\n",
    "for epoch in range(num_epochs):  # num_epochs should be the number of epochs you want to train for\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_up(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "print('Epoch [%d/%d], Loss: %.4f' % (epoch+1, num_epochs, running_loss/len(train_loader)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Model - Down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/15], Loss: 0.5800\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained model (e.g., ResNet)\n",
    "model_down = models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze all the layers in the model\n",
    "for param in model_down.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the final layer of the model to match the number of classes in the new dataset\n",
    "num_ftrs = model_down.fc.in_features\n",
    "model_down.fc = torch.nn.Linear(num_ftrs, 2)  # num_classes should be the number of classes in your new dataset\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_down.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "train_dir = \"images_with_landmarks/train/down\"  # replace train_dir with the path to your data\n",
    "\n",
    "train_data = datasets.ImageFolder(train_dir, transform=train_transforms_up)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)  # reduce batch_size\n",
    "\n",
    "\n",
    "# Training the model\n",
    "num_epochs = 15\n",
    "train_loss_history = []\n",
    "for epoch in range(num_epochs):  # num_epochs should be the number of epochs you want to train for\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_down(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "print('Epoch [%d/%d], Loss: %.4f' % (epoch+1, num_epochs, running_loss/len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "save_model(model_up, 'models/resnet18_aug_up.pth')\n",
    "save_model(model_down, 'models/resnet18_aug_down.pth')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test model - Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 84.04 %\n",
      "Class predictions: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1]\n",
      "Class predictions: ['correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'wrong_seq', 'wrong_seq', 'correct_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'correct_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'wrong_seq', 'correct_seq', 'wrong_seq', 'wrong_seq', 'correct_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'correct_seq', 'wrong_seq', 'wrong_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'wrong_seq', 'correct_seq', 'correct_seq', 'wrong_seq', 'wrong_seq', 'correct_seq', 'wrong_seq', 'wrong_seq']\n",
      "Ground-truth labels: ['correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq']\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Define the transform\n",
    "transform_test_up = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize all images to the same size\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.49097234, 0.49097234, 0.49097234], std=[0.29159072, 0.29159072, 0.29159072])\n",
    "])\n",
    "\n",
    "# Load the testing data\n",
    "test_dir = \"images_with_landmarks/train/up\"  # replace test_dir with the path to your testing data\n",
    "test_data = datasets.ImageFolder(test_dir, transform=transform_test_up)  # apply the defined transform\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=64)\n",
    "\n",
    "# Switch the model to evaluation mode\n",
    "model_up.eval()\n",
    "\n",
    "# Create a list to store predictions\n",
    "predictions = []\n",
    "\n",
    "# Test the model\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "\n",
    "        outputs = model_up(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Append the predictions to the list\n",
    "        predictions.extend(predicted.tolist())\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "accuracy = float(correct / total)\n",
    "print('Test Accuracy: %.2f %%' % (100 * accuracy))\n",
    "\n",
    "# Print the class predictions\n",
    "print('Class predictions:', predictions)\n",
    "\n",
    "# Create a reverse mapping from index to class name\n",
    "idx_to_class = {v: k for k, v in test_data.class_to_idx.items()}\n",
    "\n",
    "# Transform the predictions list\n",
    "class_predictions = [idx_to_class.get(idx, 'Unknown') for idx in predictions]\n",
    "\n",
    "# Print the class predictions\n",
    "print('Class predictions:', class_predictions)\n",
    "# Print the ground-truth labels for each image testing data\n",
    "print('Ground-truth labels:', [idx_to_class.get(idx) for idx in test_data.targets])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test model - Down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 67.07 %\n",
      "Class predictions: [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1]\n",
      "Class predictions: ['wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'correct_seq', 'wrong_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'wrong_seq', 'correct_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'wrong_seq', 'wrong_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'wrong_seq', 'wrong_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq']\n",
      "Ground-truth labels: ['correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq']\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Define the transform\n",
    "transform_test_up = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize all images to the same size\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.49058145, 0.49058145, 0.49058145], std=[0.3018868, 0.3018868, 0.3018868])\n",
    "])\n",
    "\n",
    "# Load the testing data\n",
    "test_dir = \"images_with_landmarks/train/down\"  # replace test_dir with the path to your testing data\n",
    "test_data = datasets.ImageFolder(test_dir, transform=transform_test_up)  # apply the defined transform\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=64)\n",
    "\n",
    "# Switch the model to evaluation mode\n",
    "model_up.eval()\n",
    "\n",
    "# Create a list to store predictions\n",
    "predictions = []\n",
    "\n",
    "# Test the model\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "\n",
    "        outputs = model_up(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Append the predictions to the list\n",
    "        predictions.extend(predicted.tolist())\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "accuracy = float(correct / total)\n",
    "print('Test Accuracy: %.2f %%' % (100 * accuracy))\n",
    "\n",
    "# Print the class predictions\n",
    "print('Class predictions:', predictions)\n",
    "\n",
    "# Create a reverse mapping from index to class name\n",
    "idx_to_class = {v: k for k, v in test_data.class_to_idx.items()}\n",
    "\n",
    "# Transform the predictions list\n",
    "class_predictions = [idx_to_class.get(idx, 'Unknown') for idx in predictions]\n",
    "\n",
    "# Print the class predictions\n",
    "print('Class predictions:', class_predictions)\n",
    "# Print the ground-truth labels for each image testing data\n",
    "print('Ground-truth labels:', [idx_to_class.get(idx) for idx in test_data.targets])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
