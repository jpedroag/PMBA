{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from mediapipe import solutions\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Video indexes for the correct and incorrect ones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_index_corr = [1, 2, 3, 4, 5, 6, 7, 47, 78, 79, 80, 83, 85, 100, 101, 102, 113, 114, 115, 116, 127, 129, 131, 132, 133\n",
    "                , 134, 135, 136, 137, 138, 140, 141, 142, 144, 146, 147, 148, 162, 163, 164, 165, 173, 174, 175, 177, 178, 186, 187, 188, 197]\n",
    "\n",
    "video_index_incorr = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 36, 37, 42, 44,\n",
    "                46, 55,56,57,58,81,104,105, 107, 108, 110, 149, 150, 151, 152, 153, 155, 156, 166, 167, 168, 171, 198]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dataset with landamarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image processing completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "\n",
    "# Define the pose landmarking model\n",
    "model_path = 'pose_landmarker_heavy.task'\n",
    "\n",
    "# Path to the folder containing the images\n",
    "image_folder = 'images/wrong_seq/'\n",
    "\n",
    "# Define the output folder to save the processed images\n",
    "output_folder = 'images_with_landmarks/wrong_seq'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "BaseOptions = mp.tasks.BaseOptions\n",
    "PoseLandmarker = mp.tasks.vision.PoseLandmarker\n",
    "PoseLandmarkerOptions = mp.tasks.vision.PoseLandmarkerOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "options = PoseLandmarkerOptions(\n",
    "    base_options=BaseOptions(model_asset_path=model_path),\n",
    "    running_mode=VisionRunningMode.IMAGE)\n",
    "\n",
    "with PoseLandmarker.create_from_options(options) as landmarker:\n",
    "    # Iterate over the images in the folder\n",
    "    for filename in os.listdir(image_folder):\n",
    "        # Read the image using OpenCV\n",
    "        image_path = os.path.join(image_folder, filename)\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        # Perform pose landmarking on the image\n",
    "        image = mp.Image.create_from_file(image_path)\n",
    "        pose_landmarker_result = landmarker.detect(image)\n",
    "\n",
    "        pose_landmarks_list = pose_landmarker_result.pose_landmarks\n",
    "\n",
    "        # Check if pose landmarks are detected\n",
    "        if pose_landmarks_list is not None:\n",
    "            annotated_image = np.copy(image.numpy_view())\n",
    "            # Loop through the detected poses to visualize.\n",
    "            for idx in range(len(pose_landmarks_list)):\n",
    "                pose_landmarks = pose_landmarks_list[idx]\n",
    "\n",
    "                # Draw the pose landmarks.\n",
    "                pose_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "                pose_landmarks_proto.landmark.extend([\n",
    "                    landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in pose_landmarks\n",
    "                ])\n",
    "\n",
    "                solutions.drawing_utils.draw_landmarks(\n",
    "                    annotated_image,\n",
    "                    pose_landmarks_proto,\n",
    "                    solutions.pose.POSE_CONNECTIONS,\n",
    "                    solutions.drawing_styles.get_default_pose_landmarks_style())\n",
    "\n",
    "            # Save the processed image to the output folder\n",
    "            output_path = os.path.join(output_folder, filename)\n",
    "            plt.imsave(output_path, annotated_image)\n",
    "        else:\n",
    "            print(f'No pose landmarks detected in {filename}.')\n",
    "\n",
    "print('Image processing completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.49079132\n",
      "Standard Deviation: 0.29643208\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "\n",
    "train_dir = \"images_with_landmarks\"  # Replace with the path to your training data\n",
    "\n",
    "# Create the training dataset\n",
    "train_data = datasets.ImageFolder(train_dir, transform=transforms.ToTensor())\n",
    "\n",
    "# Iterate over the dataset and collect pixel values\n",
    "pixel_values = []\n",
    "for image, _ in train_data:\n",
    "    image = np.array(image)  # Convert the image to a NumPy array\n",
    "    pixel_values.append(image.flatten())\n",
    "\n",
    "# Calculate mean and standard deviation\n",
    "pixel_values = np.concatenate(pixel_values, axis=0)\n",
    "mean = np.mean(pixel_values, axis=0)\n",
    "std = np.std(pixel_values, axis=0)\n",
    "\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Standard Deviation:\", std)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data augmentation and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.49079132, 0.49079132, 0.49079132], std=[0.29643208, 0.29643208, 0.29643208])\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/14], Loss: 0.7566\n",
      "Epoch [2/14], Loss: 0.7227\n",
      "Epoch [3/14], Loss: 0.6959\n",
      "Epoch [4/14], Loss: 0.6797\n",
      "Epoch [5/14], Loss: 0.6271\n",
      "Epoch [6/14], Loss: 0.6100\n",
      "Epoch [7/14], Loss: 0.5473\n",
      "Epoch [8/14], Loss: 0.5323\n",
      "Epoch [9/14], Loss: 0.5175\n",
      "Epoch [10/14], Loss: 0.5254\n",
      "Epoch [11/14], Loss: 0.5062\n",
      "Epoch [12/14], Loss: 0.4708\n",
      "Epoch [13/14], Loss: 0.4707\n",
      "Epoch [14/14], Loss: 0.4560\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained model (e.g., ResNet)\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze all the layers in the model\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the final layer of the model to match the number of classes in the new dataset\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(num_ftrs, 2)  # num_classes should be the number of classes in your new dataset\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "train_dir = \"images_with_landmarks\"  # replace train_dir with the path to your data\n",
    "\n",
    "train_data = datasets.ImageFolder(train_dir, transform=train_transforms)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)  # reduce batch_size\n",
    "\n",
    "\n",
    "# Training the model\n",
    "num_epochs = 14\n",
    "train_loss_history = []\n",
    "for epoch in range(num_epochs):  # num_epochs should be the number of epochs you want to train for\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print('Epoch [%d/%d], Loss: %.4f' % (epoch+1, num_epochs, running_loss/len(train_loader)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save or load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the state of the model\n",
    "torch.save(model.state_dict(), 'models/resnet18_aug.pth')\n",
    "\n",
    "#model.load_state_dict(torch.load('models/resnet18_aug.pth'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 62.50 %\n",
      "Class predictions: [0, 0, 0, 1, 1, 0, 1, 0]\n",
      "Class predictions: ['correct_seq', 'correct_seq', 'correct_seq', 'wrong_seq', 'wrong_seq', 'correct_seq', 'wrong_seq', 'correct_seq']\n",
      "Ground-truth labels: ['correct_seq', 'correct_seq', 'correct_seq', 'correct_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq', 'wrong_seq']\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Define the transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize all images to the same size\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "])\n",
    "\n",
    "# Load the testing data\n",
    "test_dir = \"test_dataset\"  # replace test_dir with the path to your testing data\n",
    "test_data = datasets.ImageFolder(test_dir, transform=transform)  # apply the defined transform\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=64)\n",
    "\n",
    "# Switch the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Create a list to store predictions\n",
    "predictions = []\n",
    "\n",
    "# Test the model\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Append the predictions to the list\n",
    "        predictions.extend(predicted.tolist())\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "accuracy = float(correct / total)\n",
    "print('Test Accuracy: %.2f %%' % (100 * accuracy))\n",
    "\n",
    "# Print the class predictions\n",
    "print('Class predictions:', predictions)\n",
    "\n",
    "# Create a reverse mapping from index to class name\n",
    "idx_to_class = {v: k for k, v in test_data.class_to_idx.items()}\n",
    "\n",
    "# Transform the predictions list\n",
    "class_predictions = [idx_to_class.get(idx, 'Unknown') for idx in predictions]\n",
    "\n",
    "# Print the class predictions\n",
    "print('Class predictions:', class_predictions)\n",
    "# Print the ground-truth labels for each image testing data\n",
    "print('Ground-truth labels:', [idx_to_class.get(idx) for idx in test_data.targets])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
